# hdfs에 있는 파일 뽑아내기

- bin/hdfs dfs -get airline_dep_delay_output1/part-r-00000 ../workspace/0824/
  - airline_dep_delay_output1/part-r-00000에 있는 파일을 
  - ../workspace/0824/에 -get한다.
- 그런데 파일이 한블럭인 128MB를 넘는다면,
- 여러개의 파일이 생긴다.



## 여러개 가져오기

- bin/hdfs dfs -get airline_dep_delay_output1/part-r-* ../workspace/0824/
  - 와일드 카드 사용.
  - part-r-*



## 합쳐서 하나로 가져오고 싶을 때

- bin/hdfs dfs -getmerge airline_dep_delay_output1 ../workspace/0824/
  - -getmerge
  - 대신 전송받는 디렉토리 끝에 merge될 파일 이름을 정해주어야한다.



## 도착지연 건수 추출

- 2012,1	149036
  2012,10	188527
  2012,11	150009
  2012,12	201482
  2012,2	138349
  2012,3	183040
  2012,4	150954
  2012,5	171789
  2012,6	197091
  2012,7	224610
  2012,8	208734
  2012,9	162147
- 그런데 출발지연 건수를 뽑을 때 이미 파서에 출발지연 항목도 넣어 놓았다.
- 그래서 파서와 리듀서는 그대로 사용가능하고 바꿔줘야할 부분이 카운트(메인)와 매퍼 부분이다.
- 원래는 getDepartureDelayTime이었던 부분을 getArriveDelayTime로 바꾸고 
- 0보다 큰 값을 키와 밸류로 나눠준다.
- 메인 코드도 교체 해준다.

### 그런데 도착지연 출발지연을 한방에 할 수는 없을까?

## 헬퍼 클래스, GenericOptionParser, Tool, ToolRunner를 사용해보자.

- DelayCount -> Driver

~~~ java
package com.tazo.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.tazo.hadoop.mapper.DelayCountMapper;
import com.tazo.hadoop.reducer.DelayCountReducer;

public class DelayCount extends Configured implements Tool{
	public static void main(String[] args) throws Exception{
		int res = ToolRunner.run(new Configuration(), new DelayCount(), args);
		System.out.println("MR-Job Result: " + res);
	}

	
	// Override run, Tool의 메소드 구현 
	@Override
	public int run(String[] args) throws Exception {
		
		// GenericOptionsParser에서 제공하는 파라미터를 제외한 나머지 파라미터 가져오기. 
		// input, output 가져온다는 것이다. 
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		
		if(otherArgs.length !=2) {
			System.out.println("Usage: DelayCount <in> <out>");
			System.exit(2);
		}
		
		
		Job job = Job.getInstance(getConf(), "DelayCount");
		
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		job.setJarByClass(DelayCount.class);
		job.setMapperClass(DelayCountMapper.class);
		job.setReducerClass(DelayCountReducer.class);	
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);

		
		job.waitForCompletion(true);
		
		return 0;
	}	
}
~~~



- DelayCountMapper

~~~ java
package com.tazo.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.tazo.hadoop.common.AirlinePerformanceParser;

public class DelayCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

	// 들어오는 값에 따라서 폼을 바꾸기 위해 workType을 선언한다.
	private String workType;
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputkey = new Text();

	// Override setup
	@Override
	protected void setup(Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {

		// setup()은 매퍼가 실행 될 때 단 한번만 실행.
		workType = context.getConfiguration().get("workType");
	}

	// Override map
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {

		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);

		outputkey.set(parser.getYear() + "," + parser.getMonth());

		if (workType.equals("departure")) {
			if (parser.getDepartureDelayTime() > 0) {
				context.write(outputkey, outputValue);
			}
		} else if (workType.equals("Arrival")) {
			if (parser.getArriveDelayTime() > 0) {
				context.write(outputkey, outputValue);
			}
		}
	}
}
~~~







### 나머지 클래스는 똑같음.

### 하둡으로 와서

- bin/yarn jar /mnt/Share/java/com.airline-0.0.4.jar com.tazo.hadoop.driver.DelayCount -D workType=departure airline_dep_delay_input dep_delay_output
  - 패키지, 메인 클래스 이름 뒤에 -D의 옵션을 주고 만들어 놓았던 workType에서 departure를 주고
  - 하던대로 하면 된다.



- 하둡은 맵리듀스 Job의 진행 상황을 모니터링 할 수 있게 Counter라는 API를 제공

- 모든 Job은 다수의 내장 카운터를 가지고 있음

- 내장 카운터는 맵, 리듀스 콤바이너의 입출력 레코드 선수와 바이드를 확인하며, 몇개의 맵 태스크와 리듀스 태스크가 실행되고 실패 했는지, 파일 시스템에서는 얼마나 많은 바이트를 읽고 썼는지에 대한 정보를 제공

- 이러한 내장 카운터의 값은 Job을 실행하면 콘솔 화면에 출력되는 로그에 나타남



### 위 내용으로 보아 사용자 정의 카운터를 만들어보자.

- 중간에 원하는 값을 더 뽑아 데이터 분석을 쉽게 유연하게 유용하게 하기 위해.

~~~ java
package com.tazo.hadoop.counter;

public enum DelayCounters {
	
	// 로그를 만들기 위해 선언. 
	not_available_arrival, scheduled_arrival, early_arrival, 
	not_departure_arrival, scheduled_departure, early_departure;
	}

~~~

- 이넘 클래스로 로그를 선언해준다.

~~~java
package com.tazo.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.tazo.hadoop.common.AirlinePerformanceParser;
import com.tazo.hadoop.counter.DelayCounters;

public class DelayCountMapperWithCounter extends Mapper<LongWritable, Text, Text, IntWritable> {

	private String workType;
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputkey = new Text();

	@Override
	protected void setup(Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		workType = context.getConfiguration().get("workType");
	}

	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);

		if (workType.equals("departure")) {
			if (parser.isDepartureDelayAvailable()) {
				if (parser.getDepartureDelayTime() > 0) {
					outputkey.set(parser.getYear() + "," + parser.getMonth());
					context.write(outputkey, outputValue);
				} else if (parser.getDepartureDelayTime() == 0) {
					context.getCounter(DelayCounters.scheduled_departure).increment(1);
				} else if (parser.getDepartureDelayTime() < 0) {
					context.getCounter(DelayCounters.early_departure).increment(1);
				}
			} else {
				context.getCounter(DelayCounters.not_available_departure).increment(1);
			}

		} else if (workType.equals("arrival")) {
			if (parser.isArriveDelayAvailable()) {
				if (parser.getArriveDelayTime() > 0) {
					outputkey.set(parser.getYear() + "," + parser.getMonth());
					context.write(outputkey, outputValue);
				} else if (parser.getArriveDelayTime() == 0) {
					context.getCounter(DelayCounters.scheduled_arrival).increment(1);
				} else if (parser.getArriveDelayTime() < 0) {
					context.getCounter(DelayCounters.early_arrival).increment(1);
				}
			} else {
				context.getCounter(DelayCounters.not_available_arrival).increment(1);
			}
		}
	}
}
~~~





## 동시에 출발과 도착 지연을 같이 해보자

## 병렬처리로 코딩해보자.



### 1. Multiple outputs

a. org.apache.hadoop.mapriduce.lib.output.MultipleOutputs는 여러개의 출력 데이터를 쉽게 생성할 수 있는 기능을 제공

b. MultipleOutputs은 여러개의 OutputCollectors를 만들고 각 OutputCollectors에 대한 출력 경로, 출력 포멧, 키와 값 유형을 설정

c. 이러한 파라미터 설정은 MultipleOutputs에서 제공하는 static 메서드 addNamedOutput을 호출해 설정

d. MultipleOutputs에서 출력하는 데이터는 기존 맵리듀스 잡에서 생성하는 데이터와는 별개로 생성

e. 맵리듀스 잡이 종료되면 리듀스 단계에서 part-r-xxxxx라는 출력 데이터를 생성함. 그런데 리듀스 단계에서 MultipleOutputs를 이용햐 myfile이라는 디렉터리에 데이터를 생성할 경우 part-r-xxxxx와 myfile-r-xxxxx가 동시에 생성.



- 파서는 똑같음
- 딜레이 카운터도 똑같음.



#### 1. driver

~~~ java
package com.tazo.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.tazo.hadoop.mapper.DelayCountMapperWithMultipleOutputs;
import com.tazo.hadoop.reducer.DelayCountReducerWithMultipleOutputs;

public class DelayCountWithMultipleOutputs extends Configured implements Tool {
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new DelayCountWithMultipleOutputs(), args);
		System.out.println("MR-Job Result: " + res);
	}

	@Override
	public int run(String[] args) throws Exception {
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();

		if (otherArgs.length != 2) {
			System.out.println("Usage: DelayCountWithMultipleOutputs <in> <out>");
			System.exit(2);
		}

		// job에 이름을 붙혀주는 것일 뿐.
		Job job = Job.getInstance(getConf(), "DelayCountWithMultipleOutputs");

		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));

		job.setJarByClass(DelayCountWithMultipleOutputs.class);
		job.setMapperClass(DelayCountMapperWithMultipleOutputs.class);
		job.setReducerClass(DelayCountReducerWithMultipleOutputs.class);

		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		MultipleOutputs.addNamedOutput(job, "departure", TextOutputFormat.class, Text.class, IntWritable.class);
		MultipleOutputs.addNamedOutput(job, "arrival", TextOutputFormat.class, Text.class, IntWritable.class);

		job.waitForCompletion(true);
		return 0;
	}
}
~~~



#### 2. Mapper class

~~~ java
package com.tazo.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.tazo.hadoop.common.AirlinePerformanceParser;
import com.tazo.hadoop.counter.DelayCounters;

public class DelayCountMapperWithMultipleOutputs extends Mapper<LongWritable, Text, Text, IntWritable> {
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputkey = new Text();

	@Override
	protected void setup(Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
	}

	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {

		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);

		if (parser.isDepartureDelayAvailable()) {
			if (parser.getDepartureDelayTime() > 0) {
				outputkey.set("D," + parser.getYear() + "," + parser.getMonth());
				context.write(outputkey, outputValue);
			} else if (parser.getDepartureDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_departure).increment(1);
			} else if (parser.getDepartureDelayTime() < 0) {
				context.getCounter(DelayCounters.early_departure).increment(1);
			}
		} else {
			context.getCounter(DelayCounters.not_available_departure).increment(1);
		}

		if (parser.isArriveDelayAvailable()) {
			if (parser.getArriveDelayTime() > 0) {
				outputkey.set("A," + parser.getYear() + "," + parser.getMonth());
				context.write(outputkey, outputValue);
			} else if (parser.getArriveDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_arrival).increment(1);
			} else if (parser.getArriveDelayTime() < 0) {
				context.getCounter(DelayCounters.early_arrival).increment(1);
			}
		} else {
			context.getCounter(DelayCounters.not_available_arrival).increment(1);
		}
	}
}
~~~





#### 3. Reducer class

~~~ java
package com.tazo.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

public class DelayCountReducerWithMultipleOutputs extends Reducer<Text, IntWritable, Text, IntWritable> {

	private MultipleOutputs<Text, IntWritable> mos;
	private Text outputkey = new Text();
	private IntWritable result = new IntWritable();

	@Override
	protected void setup(Reducer<Text, IntWritable, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		mos = new MultipleOutputs<Text, IntWritable>(context);
	}

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		String[] columns = key.toString().split(",");

		outputkey.set(columns[1] + "," + columns[2]);

		int sum = 0;

		for (IntWritable value : values) {
			sum += value.get();
		}

		result.set(sum);

		if (columns[0].equals("D")) {
			mos.write("departure", outputkey, result); // 출력 디렉터리명 추가
		} else
			mos.write("arrival", outputkey, result);
	}

	
	// MultipleOutputs 리소스가 잡고있는 객체를 클로즈 해준다. 
	@Override
	protected void cleanup(Reducer<Text, IntWritable, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		mos.close();
	}
}
~~~



#### 4. Hadoop

- bin/yarn jar /mnt/Share/java/com.airline-0.0.7.jar com.tazo.hadoop.driver.DelayCountWithMultipleOutputs airline_dep_delay_input delay_counts_mos01

- export

  [companion_tazo@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls delay_counts_mos01
  Found 4 items
  companion_tazo supergroup          0 2020-08-24 15:46 delay_counts_mos01/_SUCCESS
  companion_tazo supergroup        171 2020-08-24 15:46 delay_counts_mos01/arrival-r-00000
  companion_tazo supergroup        171 2020-08-24 15:46 delay_counts_mos01/departure-r-00000
  companion_tazo supergroup          0 2020-08-24 15:46 delay_counts_mos01/part-r-00000



##### MultipleOutputs 정리

###### 1. Parser

- 파서를 알기 위해선 파싱을 먼저 알아야하는데 파싱이란 들어온 텍스트를 선별하는 부분이고
- 파서는 그것들을 골라주는 부분이다.

~~~ java
package com.tazo.hadoop.common;

import org.apache.hadoop.io.Text;

public class AirlinePerformanceParser {
	// 쓸 필드를 선정해보자.
	
  // 원하는 값을 나열.
	private int year;
	private int month;
	private int day;

  // 인트인 부분을 0으로 초기화
	private int arriveDelayTime = 0;
	private int departureDelayTime = 0;
	private int distance = 0;
	
  
	private boolean arriveDelayAvailable = true;
	private boolean departureDelayAvailable = true;
	private boolean distanceAvailable = true;
	
	private String uniqueCarrier;
	
	// AirlinePerformanceParser 하둡의 Text형식의 인자를 받는다.
	public AirlinePerformanceParser(Text text) {
		try {
      // 값이 넘어오면 "," 기준으로 스플릿하고 text형식으로 columns에 넣는다.
			String[] columns = text.toString().split(",");
			
      // String인 데이터의 값을 parseInt하여 int로 casting 후 값을 추가.
			year = Integer.parseInt(columns[0]);
			month = Integer.parseInt(columns[1]);
			day = Integer.parseInt(columns[2]);
			uniqueCarrier = columns[5];
			
			if(!columns[16].equals("")) {
				departureDelayTime = (int)Float.parseFloat(columns[16]);
			} else {
				departureDelayAvailable = false;
			}
			
			if(!columns[26].equals("")) {
				arriveDelayTime = (int)Float.parseFloat(columns[26]);
			} else {
				arriveDelayAvailable = false;
			}
			
			if(!columns[37].equals("")) {
				distance = (int)Float.parseFloat(columns[37]);
			} else {
				 distanceAvailable= false;
			}
			
		} catch (Exception e) {
			System.out.println("Error parsing a record : " + e.getMessage());
		}
	}
  
  // 게터 부분.

~~~





###### 2. Mapper

~~~java
package com.tazo.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.tazo.hadoop.common.AirlinePerformanceParser;
import com.tazo.hadoop.counter.DelayCounters;

// DelayCountMapperWithMultipleOutputs는 Mapper<LongWritable, Text, Text, IntWritable>를
// 상속받고 Mapper는 입력은 LongWritable, Text 출력은 Text, IntWritable 해준다.
public class DelayCountMapperWithMultipleOutputs extends Mapper<LongWritable, Text, Text, IntWritable> {
  
  // 그리고 맵핑을 하면 기본적으로 값이 1이기 때문에 outputValue로 상수 1을 선언해준다. 
  // 파싱하는 도중 값이 0이 되는 것은 없기 때문.
  // Text타입인 outputkey 객체를 선언.
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputkey = new Text();


  // map을 오버라이드 한다. LongWritable key, Text value로 받고
  // Mapper<LongWritable, Text, Text, IntWritable>.Context context의 값이 들어간다. 
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {

    // 파서 클래스 객체를 생성.
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);

    // 파서 클래스의 isDepartureDelayAvailable true일 경우 실행.
    // if문 통과.
    // 만약 getDepartureDelayTime이 0보다 크면 outputkey에 D와 년도, 월을 입력
    // 그런다음 context의 write함수를 써서 값을 입력하는데 키 밸류로 들어간다. 
    // 연도와 월이 키 그리고 상수로 지정했던 1이 밸류로 들어감.
    // 그리고 getDepartureDelayTime이 0과 같거나 작다면 상수로 정해 놓았던 값들에
    // 1씩 증가 시켜 준다.
		if (parser.isDepartureDelayAvailable()) {
			if (parser.getDepartureDelayTime() > 0) {
				outputkey.set("D," + parser.getYear() + "," + parser.getMonth());
				context.write(outputkey, outputValue);
			} else if (parser.getDepartureDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_departure).increment(1);
			} else if (parser.getDepartureDelayTime() < 0) {
				context.getCounter(DelayCounters.early_departure).increment(1);
			}
      // 아니라면 DelayCounters.not_available_departure을 1 증가시켜줌.
		} else {
			context.getCounter(DelayCounters.not_available_departure).increment(1);
		}
	
    // 마찬가지로 위와 동일한 행동을 함.
		if (parser.isArriveDelayAvailable()) {
			if (parser.getArriveDelayTime() > 0) {
				outputkey.set("A," + parser.getYear() + "," + parser.getMonth());
				context.write(outputkey, outputValue);
			} else if (parser.getArriveDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_arrival).increment(1);
			} else if (parser.getArriveDelayTime() < 0) {
				context.getCounter(DelayCounters.early_arrival).increment(1);
			}
		} else {
			context.getCounter(DelayCounters.not_available_arrival).increment(1);
		}
	}
}

~~~



###### 3. Reducer

~~~ java
package com.tazo.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

// 리듀서는 Reducer<Text, IntWritable, Text, IntWritable>의 입출력 형식.
public class DelayCountReducerWithMultipleOutputs extends Reducer<Text, IntWritable, Text, IntWritable> {

  // mos는 MultipleOutputs<Text, IntWritable>타입을 따른다.
	private MultipleOutputs<Text, IntWritable> mos;
	private Text outputkey = new Text();
	private IntWritable result = new IntWritable();

  // context를 인자로 넣을 것인데 Text와 Int의 짝을 가질 것이다.
	@Override
	protected void setup(Reducer<Text, IntWritable, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		mos = new MultipleOutputs<Text, IntWritable>(context);
	}

  // Text key, Iterable<IntWritable> values의 형식
  // 
	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
    // "," 기준으로 키에 값을 넣고 컬럼 배열에 넣는다. 
		String[] columns = key.toString().split(",");

    // 연도, 월
		outputkey.set(columns[1] + "," + columns[2]);

		int sum = 0;

		for (IntWritable value : values) {
			sum += value.get();
		}

		result.set(sum);

		if (columns[0].equals("D")) {
			mos.write("departure", outputkey, result); // 출력 디렉터리명 추가
		} else
			mos.write("arrival", outputkey, result);
	}

	
	// MultipleOutputs 리소스가 잡고있는 객체를 클로즈 해준다. 
	@Override
	protected void cleanup(Reducer<Text, IntWritable, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		mos.close();
  }	
}
~~~





##### 4. Main

~~~ java
package com.tazo.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.tazo.hadoop.mapper.DelayCountMapperWithMultipleOutputs;
import com.tazo.hadoop.reducer.DelayCountReducerWithMultipleOutputs;

public class DelayCountWithMultipleOutputs extends Configured implements Tool {
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new DelayCountWithMultipleOutputs(), args);
		System.out.println("MR-Job Result: " + res);
	}

  // tool의 메소드 run
	@Override
	public int run(String[] args) throws Exception {
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();

		if (otherArgs.length != 2) {
			System.out.println("Usage: DelayCountWithMultipleOutputs <in> <out>");
			System.exit(2);
		}

		// job에 이름을 붙혀주는 것일 뿐.
		Job job = Job.getInstance(getConf(), "DelayCountWithMultipleOutputs");

		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));

		job.setJarByClass(DelayCountWithMultipleOutputs.class);
		job.setMapperClass(DelayCountMapperWithMultipleOutputs.class);
		job.setReducerClass(DelayCountReducerWithMultipleOutputs.class);

		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		MultipleOutputs.addNamedOutput(job, "departure", TextOutputFormat.class, Text.class, IntWritable.class);
		MultipleOutputs.addNamedOutput(job, "arrival", TextOutputFormat.class, Text.class, IntWritable.class);

		job.waitForCompletion(true);
		return 0;
	}
}
~~~



