1. 출발지연

2. 도착지연

3. 카운트

4. 사용자 정의 카운트

5. 같이 뽑는거

6. 병렬처리



# Sorting, 보조정렬, secondary sort

- 기존 항공 운항 데이터 분석 결과를 보면 월 순서대로 출력되지 않았음.

- 출력 데이터의 키 값 자체가 연도와 월이 합쳐진 하나의 문자열로 인식

- 보조정렬은 키의 값들을 그룹핑하고 그룹핑된 레코드에 순서를 부여하는 방식

- 보조정렬 구현순서

  1. 기존 키의 값들을 조합한 복합키(Composite Key)를 정의, 이 때 키의 값중에서 어떤 키를 그룹핑 키로 사용할 지 결정

  2. 복합키의 레코드를 정렬하기 위한 비교기(Comparator)를 정의

  3. 그룹핑 키를 파티셔닝할 파티셔너(Partitioner)를 정의

  4. 그룹핑 키를 정렬하기 위한 비교기(Composite)를 정리



#### 연도와 월별로 분류해서 보고 싶다. 

#### 시스템 사양상 연도별 6개월 씩 선별



## 기존 키의 값들을 조합한 복합키(Composite Key)를 정의

~~~ java
package com.tazo.hadoop.common;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableUtils;

public class DateKey implements WritableComparable<DateKey> {
	
	// 복합키를 생성해보자.
	
	
	
	private String year;
	// Wrapper class 여기선 int로 해도 무방할듯. 
	private Integer month;
	
	public DateKey() {
	}
	
	public DateKey(String year, Integer month) {
		this.year = year;
		this.month = month;
	}

	public String getYear() {
		return year;
	}

	public void setYear(String year) {
		this.year = year;
	}

	public Integer getMonth() {
		return month;
	}

	public void setMonth(Integer month) {
		this.month = month;
	}

	
	//DataOutput에 year, month를 넣었다. 형변환?
	@Override
	public void write(DataOutput out) throws IOException {
		WritableUtils.writeString(out, year);
		out.writeInt(month);
	}

	
	// 
	@Override
	public void readFields(DataInput in) throws IOException {
		year = WritableUtils.readString(in);
		month = in.readInt();
	}
	
	// 복합 키끼리 비교 연도가 같으면 월을 비교. 
	@Override
	public int compareTo(DateKey key) {
		int result = year.compareTo(key.year);
		if (result == 0) {
			result = month.compareTo(key.month);
		}
		return 0;
	}

	// String 형식으로 리턴.
	@Override
	public String toString() {
		return new StringBuilder().append(year).append(",").append(month).toString();
	}	
}
~~~



## 복합키의 레코드를 정렬하기 위한 비교기(Comparator)를 정의

~~~java
package com.tazo.hadoop.common;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class DateKeyComparator extends WritableComparator{
	
	// 생성자 
	protected DateKeyComparator() {
		super(DateKey.class, true);
	}

	
	@SuppressWarnings("rawtypes")
	@Override
	public int compare(WritableComparable a, WritableComparable b) {
		DateKey k1 = (DateKey)a;
		DateKey k2 = (DateKey)b;
		
		int cmp = k1.getYear().compareTo(k2.getYear());
		if(cmp != 0) {
			return cmp;
		}
		// 월 비교하는 부분.
		return k1.getMonth() == k2.getMonth() ? 0 : (k1.getMonth() < k2.getMonth() ? -1 : 1);
	}	
}
~~~



## 그룹핑 키를 파티셔닝할 파티셔너(Partitioner)를 정의

~~~java
package com.tazo.hadoop.common;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Partitioner;

public class GroupKeyPatitioner extends Partitioner<DateKey, IntWritable>{

	
	// 그룹핑 키를 파티셔닝할 파티셔너(Partitioner)를 정의 
	// 셔플링(키가 같은 값들 끼리의 모음)할때 파티셔닝을 한다. 
	// 이 단계는 파티셔닝의 룰을 만들어주는 단계. 
	@Override
	public int getPartition(DateKey key, IntWritable value, int numPartitions) {
		int hash = key.getYear().hashCode();
		int partition = hash % numPartitions;
		return partition;
	}	
}
~~~



## 그룹핑 키를 정렬하기 위한 비교기(Composite)를 정리

~~~java
package com.tazo.hadoop.common;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class GroupKeyComparator extends WritableComparator {
	protected GroupKeyComparator() {
		super(DateKey.class, true);
	}

	@SuppressWarnings("rawtypes")
	@Override
	public int compare(WritableComparable a, WritableComparable b) {
		DateKey k1 = (DateKey)a;
		DateKey k2 = (DateKey)b;
		// 연도 비교하는 부분.
		return k1.getYear().compareTo(k2.getYear());
	}
}
~~~



## DelayCountWithDateKey, Main

~~~java
package com.tazo.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.tazo.hadoop.common.DateKey;
import com.tazo.hadoop.common.DateKeyComparator;
import com.tazo.hadoop.common.GroupKeyComparator;
import com.tazo.hadoop.common.GroupKeyPatitioner;
import com.tazo.hadoop.mapper.DelayCountMapperWithDateKey;
import com.tazo.hadoop.reducer.DelayCountReducerWithDateKey;

public class DelayCountWithDateKey extends Configured implements Tool{
	
	public static void main(String[] args) throws Exception{
		int res = ToolRunner.run(new Configuration(), new DelayCountWithDateKey(), args);
		System.out.println("MR-Job Result: " + res);
	}
	
		@Override
		public int run(String[] args) throws Exception {
			String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
			
			if(otherArgs.length !=2) {
				System.err.println("Usage: DelayCount <in> <out>");
				System.exit(2);
			}
			
			// job에 이름을 붙혀주는 것일 뿐.
			Job job = Job.getInstance(getConf(), "DelayCountWithDateKey");
			
			// 입출력 데이터 경로 설정. 
			FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
			FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
			
			// Job 클래스 설정. 
			job.setJarByClass(DelayCountWithDateKey.class);
			job.setPartitionerClass(GroupKeyPatitioner.class);
			job.setGroupingComparatorClass(GroupKeyComparator.class);
			job.setSortComparatorClass(DateKeyComparator.class);	
			
			//Mapper 클래스
			job.setMapperClass(DelayCountMapperWithDateKey.class);
			
			// Redecer 클래스
			job.setReducerClass(DelayCountReducerWithDateKey.class);
			
			job.setMapOutputKeyClass(DateKey.class);
			job.setMapOutputValueClass(IntWritable.class);

			// 입출력 데이터 포맷 설정. 
			job.setInputFormatClass(TextInputFormat.class);
			job.setOutputFormatClass(TextOutputFormat.class);
			
			// 출력키 및 출력값 유형 설정. 
			job.setOutputKeyClass(DateKey.class);
			job.setOutputValueClass(IntWritable.class);
			
			
			// MultipleOutputs 설정
			MultipleOutputs.addNamedOutput(job, "departure", TextOutputFormat.class, Text.class, IntWritable.class);
			MultipleOutputs.addNamedOutput(job, "arrival", TextOutputFormat.class, Text.class, IntWritable.class);
			
			job.waitForCompletion(true);
			return 0;
		}
	}
~~~



## DelayCountMapperWithDateKey, MapperClass

~~~ java
package com.tazo.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.tazo.hadoop.common.AirlinePerformanceParser;
import com.tazo.hadoop.common.DateKey;
import com.tazo.hadoop.counter.DelayCounters;

public class DelayCountMapperWithDateKey extends Mapper<LongWritable, Text, DateKey, IntWritable>{
	private final static IntWritable outputValue = new IntWritable(1);
	private DateKey outputKey = new DateKey();
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, DateKey, IntWritable>.Context context)
			throws IOException, InterruptedException {
		
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		if(parser.isDepartureDelayAvailable()) {
			if(parser.getDepartureDelayTime() > 0) {
				outputKey.setYear("D," + parser.getYear());
				outputKey.setMonth(parser.getMonth());
				context.write(outputKey, outputValue);
			} else if (parser.getDepartureDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_departure).increment(1);
			} else if (parser.getDepartureDelayTime() < 0) {
				context.getCounter(DelayCounters.early_departure).increment(1);
			}
		} else {
			context.getCounter(DelayCounters.not_available_departure).increment(1);
		}
		
		if(parser.isArriveDelayAvailable()) {
			if(parser.getArriveDelayTime() > 0) {
				outputKey.setYear("A," + parser.getYear());
				outputKey.setMonth(parser.getMonth());
				context.write(outputKey, outputValue);
			} else if (parser.getArriveDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_arrival).increment(1);
			} else if (parser.getArriveDelayTime() < 0) {
				context.getCounter(DelayCounters.early_arrival).increment(1);
			}
		} else {
			context.getCounter(DelayCounters.not_available_arrival).increment(1);
		}
	}	
}
~~~





## DelayCountReducerWithDateKey, Reducer

~~~java
package com.tazo.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

import com.tazo.hadoop.common.DateKey;

public class DelayCountReducerWithDateKey extends Reducer<DateKey, IntWritable, DateKey, IntWritable> {
	private MultipleOutputs<DateKey, IntWritable> mos;
	private DateKey outputKey = new DateKey();
	private IntWritable result = new IntWritable();

	@Override
	protected void setup(Reducer<DateKey, IntWritable, DateKey, IntWritable>.Context context)
			throws IOException, InterruptedException {
		mos = new MultipleOutputs<DateKey, IntWritable>(context);

	}

	@Override
	protected void reduce(DateKey key, Iterable<IntWritable> values,
			Reducer<DateKey, IntWritable, DateKey, IntWritable>.Context context)
			throws IOException, InterruptedException {

		// 콤마 구분자 분리
		String[] columns = key.getYear().split(",");

		int sum = 0;
		Integer bMonth = key.getMonth();

		if (columns[0].equals("D")) {
			for (IntWritable value : values) {
				if (bMonth != key.getMonth()) {
					result.set(sum);
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("departure", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
			}
			if (key.getMonth() == bMonth) {
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(key.getMonth());
				result.set(sum);
				mos.write("departure", outputKey, result);
			}
		} else {
			for (IntWritable value : values) {
				if (bMonth != key.getMonth()) {
					result.set(sum);
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("arrival", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
			}
			if (key.getMonth() == bMonth) {
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(key.getMonth());
				result.set(sum);
				mos.write("arrival", outputKey, result);

			}
		}
	}

	@Override
	protected void cleanup(Reducer<DateKey, IntWritable, DateKey, IntWritable>.Context context)
			throws IOException, InterruptedException {
		mos.close();
	}
}
~~~



## 리듀서

- 리듀서에는 그룹핑 파티셔너와 그룹핑 Comparator에 의해 같은 연도로 그룹핑된 데이터가 전달된 상태
- 복합키 Comparator로 인해 그룹핑된 값은 월의 순서대로 오름차순으로 정렬되어 있음.
- 하지만 리듀서 메서드에서 지연 횟수를 합산할 경우 데이터에 오류가 방생
- 예를 들어 2008년도 항공 출발 지연 데이터를 처리할 경우 다음과 같은 결과가 나타남
  - 2008 12 2647363
- 2008년도 12월만 출력되고 지연횟수도 2008년도의 모든 지연횟수가 합산되어 출력됨.
- 이러한 현상이 나타나는 이유는 리듀서는 2008년이라는 그룹키를 기준으로 연산을 수행하기 때문.
- 월별로 지연 횟수를 계산하려면 복합키를 구분해서 처리하는 코드를 구현해야함.
- 입력 데이터의 값에 해당하는 Iterable 객체를 순회할 때 월에 해단하는 값을 bMonth라는 변수에 백





### Hadoop Echo System

Pig

- latin이라는 스크립트 제공 자바를 모르는 사람이 맵 리듀스 프로그램을 쓸 수 있게 함.
- 근데 안씀. 
- 왜 안쓰냐, 기술이 좋아져서 SQL보다 Hadoop이 대세다. 
- 커리큘럼 제외

Hive

- ​	HiveQL
  - 완벽한 SQL 지원 안함.
- 왜 안쓰냐.
  - 완벽한 ANSI 표준 SQL 지원하는 것들이 나옴. 

Flume

- 플룸에 통나무가 내려옴.
  - 실시간으로 생기는 log를 실시간으로 받아서 HDFS에 쌓아주는 프로그램이다. 
- 실습하기가 까다로움.

Sqoop

- HDFS <->DB

Tajo

- 타조는 하둡 기반 데이터웨어하우스 시스템이다. 하둡 데이터 분석을 위해 일반적으로 사용되는 맵리듀스 기술 대신 관계형 데이터베이스에서 사용하는 SQL로 질의할 수 있다.
- ANSI 표준

